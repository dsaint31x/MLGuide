{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Chapter 17 of Machine Learning: An Algorithmic Perspective (2nd Edition)\n",
    "# by Stephen Marsland (http://stephenmonika.net)\n",
    "\n",
    "# You are free to use, change, or redistribute the code in any way you wish for\n",
    "# non-commercial purposes, but please maintain the name of the original author.\n",
    "# This code comes with no warranty of any kind.\n",
    "\n",
    "# Stephen Marsland, 2014\n",
    "\n",
    "import numpy as np\n",
    "#import pylab as pl\n",
    "\n",
    "class ds_rbm:\n",
    "    def __init__(self,nvisible,nhidden,nlabels=None,inputs=[],\n",
    "                 eta=0.1,momentum=0.9,decay=0.,nCDsteps=5,nepochs=1000):\n",
    "        self.nvisible = nvisible\n",
    "        self.nhidden = nhidden\n",
    "        \n",
    "        self.weights = np.random.randn(nvisible,nhidden)*0.1\n",
    "        self.visiblebias = np.random.randn(nvisible)*0.1\n",
    "        self.hiddenbias = np.random.randn(nhidden)*0.1\n",
    "        \n",
    "        self.nlabels = nlabels\n",
    "        \n",
    "        if nlabels is not None:\n",
    "            self.labelweights = np.random.randn(nlabels,nhidden)*0.1\n",
    "            self.labelbias = np.random.randn(nlabels)*0.1\n",
    "\n",
    "        # cf Hinton 8.1\n",
    "        if np.shape(inputs)[0]>0:\n",
    "            p = np.sum(inputs,axis=1)/(np.shape(inputs)[1])\n",
    "            self.visiblebias = np.log(p/(1-p))\n",
    "\n",
    "        self.eta = eta\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "        self.nCDsteps = nCDsteps\n",
    "        self.nepochs = nepochs\n",
    "\n",
    "    # Sample the visible variables\n",
    "    def compute_visible(self,hidden):\n",
    "        # Compute p(v=1|h,W), p(l=1|h,W)\n",
    "        sumin = self.visiblebias + np.dot(hidden,self.weights.T)\n",
    "\n",
    "        # Compute visible node activations\n",
    "        self.visibleprob = 1./(1. + np.exp(-sumin))\n",
    "        self.visibleact = (\n",
    "            self.visibleprob>np.random.rand(\n",
    "                np.shape(self.visibleprob)[0],self.nvisible\n",
    "            )).astype('float')\n",
    "\n",
    "        # Compute label activations (softmax)\n",
    "        if self.nlabels is not None:\n",
    "            sumin = self.labelbias + np.dot(hidden,self.labelweights.T)\n",
    "            summax = sumin.max(axis=1)\n",
    "            summax = np.reshape(summax,summax.shape+(1,)).repeat(np.shape(sumin)[1],axis=-1)            \n",
    "            sumin -= summax\n",
    "            normalisers = np.exp(sumin).sum(axis=1)\n",
    "            normalisers = np.reshape(normalisers,normalisers.shape+(1,)).repeat(np.shape(sumin)[1],axis=-1)            \n",
    "            self.labelact = np.exp(sumin)/normalisers\n",
    "\n",
    "        return [self.visibleprob,self.visibleact,self.labelact]\n",
    "\n",
    "    # Sample the hidden variables\n",
    "    def compute_hidden(self,visible,label=[]):\n",
    "        \n",
    "        # Compute p(h=1|v,W,{l})\n",
    "        if self.nlabels is not None:\n",
    "            sumin = self.hiddenbias + np.dot(visible,self.weights) + np.dot(label,self.labelweights)\n",
    "        else:\n",
    "            sumin = self.hiddenbias + np.dot(visible,self.weights)\n",
    "        self.hiddenprob = 1./(1. + np.exp(-sumin))\n",
    "        \n",
    "        # num of data x num of hidden\n",
    "        self.hiddenact = (\n",
    "            self.hiddenprob>np.random.rand(\n",
    "                np.shape(self.hiddenprob)[0],self.nhidden\n",
    "            )).astype('float')\n",
    "        return [self.hiddenact,self.hiddenprob]\n",
    "\n",
    "    def classify(self,inputs,labels):\n",
    "        h, ph = self.compute_hidden(inputs,labels)\n",
    "        #h, ph = self.compute_hidden(inputs,1./(labels.max()+1)*np.ones(np.shape(labels)))\n",
    "        v, pv, l = self.compute_visible(h)\n",
    "        \n",
    "        print ( l.argmax(axis=1) )\n",
    "        print (labels.argmax(axis=1))\n",
    "        \n",
    "        print ('Errors:', (l.argmax(axis=1) != labels.argmax(axis=1)).sum())               \n",
    "\n",
    "    def energy(self, visible, hidden, labels=[]):\n",
    "        if self.nlabels is not None:\n",
    "            ret_en = (\n",
    "                -np.dot(visible, self.visiblebias) \n",
    "                -np.dot(hidden , self.hiddenbias) \n",
    "                -np.dot(labels , self.labelbias) \n",
    "                - (np.dot(visible, self.weights     )*hidden).sum(axis=1) \n",
    "                - (np.dot(labels , self.labelweights)*hidden).sum(axis=1)\n",
    "            )\n",
    "            return ret_en\n",
    "        else:\n",
    "            \n",
    "            # (num_data x num_dimension) x (num_dimension x num_hidden) \n",
    "            # * (num_data x num_hidden)\n",
    "            ret_en = (\n",
    "                -np.dot(visible, self.visiblebias) \n",
    "                -np.dot(hidden , self.hiddenbias) \n",
    "                -(np.dot(visible, self.weights)*hidden).sum(axis=1)\n",
    "            )\n",
    "            # dsaint31's debug\n",
    "            # print(visible.shape)\n",
    "            # print(self.weights.shape)\n",
    "            # print(hidden.shape)\n",
    "            return ret_en \n",
    "\n",
    "    def contrastive_divergence(self,inputs,labels=None,\n",
    "                               dw=None,dwl=None,dwvb=None,\n",
    "                               dwhb=None,dwlb=None,silent=False):\n",
    "\n",
    "        # Clamp input into visible nodes\n",
    "        visible = inputs\n",
    "        \n",
    "        #self.visibleact = inputs\n",
    "        self.labelact = labels\n",
    "        \n",
    "        dw = 0. if dw is None else dw\n",
    "        \n",
    "        dwl = 0. if dwl is None else dwl\n",
    "        dwvb = 0. if dwvb is None else dwvb\n",
    "        dwhb = 0. if dwhb is None else dwhb\n",
    "        dwlb = 0. if dwlb is None else dwlb\n",
    "        \n",
    "        for epoch in range(self.nepochs):\n",
    "            \n",
    "            # dsaint31 debug\n",
    "            #print(epoch,\"[visible_bias:\",self.visiblebias,\"|hidden_bias:\",self.hiddenbias,\"]\")\n",
    "            \n",
    "            # Sample the hidden variables\n",
    "            self.compute_hidden(visible,labels)\n",
    "    \n",
    "            # Compute <vh>_0\n",
    "            positive = np.dot(inputs.T,self.hiddenact)            \n",
    "            #positive = np.dot(inputs.T,self.hiddenprob)\n",
    "            \n",
    "            # Contrastive Divergence : Positive_Visible_Bias\n",
    "            positive_vb = inputs.sum(axis=0)\n",
    "            \n",
    "            # Contrastive Divergence : Positive_Hidden_Bias\n",
    "            positive_hb = self.hiddenprob.sum(axis=0)\n",
    "            \n",
    "            if self.nlabels is not None:\n",
    "                positive_labels = np.dot(labels.T,self.hiddenact)\n",
    "                #positive_labels = np.dot(labels.T,self.hiddenprob)\n",
    "                positivelb = labels.sum(axis=0)\n",
    "\n",
    "            ###################################\n",
    "            # Asleep phase\n",
    "            # Do limited Gibbs sampling to sample from the hidden distribution\n",
    "            for j in range(self.nCDsteps):    \n",
    "                self.compute_visible(self.hiddenact)\n",
    "                #print(\"labelact:\"+str(self.labelact))\n",
    "                self.compute_hidden(self.visibleact,self.labelact)\n",
    "\n",
    "            # Compute <vh>_n\n",
    "            negative = np.dot(self.visibleact.T,self.hiddenact)\n",
    "            #negative = np.dot(self.visibleact.T,self.hiddenprob)\n",
    "            \n",
    "            # Contrastive Divergence : Negative_Visible_Bias\n",
    "            negative_vb = self.visibleact.sum(axis=0)\n",
    "            \n",
    "            # Contrastive Divergence : Negative_Hidden_Bias\n",
    "            negative_hb = self.hiddenprob.sum(axis=0)\n",
    "\n",
    "            if self.nlabels is not None:\n",
    "                negative_labels = np.dot(self.labelact.T,self.hiddenact)\n",
    "                #negative_labels = np.dot(self.labelact.T,self.hiddenprob)\n",
    "                negative_lb = self.labelact.sum(axis=0)\n",
    "                dwl = self.eta * ((positive_labels - negative_labels) / np.shape(inputs)[0] - self.decay*self.labelweights) + self.momentum*dwl\n",
    "                self.labelweights += dwl\n",
    "                dwlb = self.eta * (positivelb - negative_lb) / np.shape(inputs)[0] + self.momentum*dwlb\n",
    "                self.labelbias += dwlb\n",
    "\n",
    "            ###########################################\n",
    "            # Learning rule (with momentum)\n",
    "            # - weights\n",
    "            # - visiblebias\n",
    "            # - hiddenbias\n",
    "            dw = self.eta * (\n",
    "                (positive - negative) / np.shape(inputs)[0] \n",
    "                - self.decay*self.weights) + self.momentum*dw\n",
    "            self.weights += dw\n",
    "\n",
    "            dwvb = self.eta * ((positive_vb - negative_vb) / np.shape(inputs)[0] \n",
    "                              )+ self.momentum*dwvb\n",
    "            self.visiblebias += dwvb\n",
    "            \n",
    "            dwhb = self.eta * ((positive_hb - negative_hb) / np.shape(inputs)[0]\n",
    "                              ) + self.momentum*dwhb\n",
    "            self.hiddenbias += dwhb\n",
    "\n",
    "            # error = sum of square\n",
    "            error = np.sum((inputs - self.visibleact)**2)\n",
    "            \n",
    "            if (epoch%50==0) and not silent: \n",
    "                print (epoch, error/np.shape(inputs)[0], self.energy(visible,self.hiddenprob,labels).sum())\n",
    "\n",
    "            # for Awake    \n",
    "            visible = inputs\n",
    "            self.labelact = labels\n",
    "\n",
    "        if not silent:\n",
    "            self.compute_hidden(inputs,labels)\n",
    "            #print (\"energy:\",self.energy(visible,self.hiddenprob,labels).sum())\n",
    "            print (\"energy:\",self.energy(visible,self.hiddenprob,labels))\n",
    "        return error \n",
    "\n",
    "\n",
    "    def cddemo(self,inputs,labels=None,dw=None,dwl=None,dwvb=None,dwhb=None,dwlb=None,silent=False):\n",
    "\n",
    "        # Clamp input into visible nodes\n",
    "        visible = inputs\n",
    "        #self.visibleact = inputs\n",
    "        self.labelact = labels\n",
    "        \n",
    "        dw = 0. if dw is None else dw\n",
    "        dwl = 0. if dwl is None else dwl\n",
    "        dwvb = 0. if dwvb is None else dwvb\n",
    "        dwhb = 0. if dwhb is None else dwhb\n",
    "        dwlb = 0. if dwlb is None else dwlb\n",
    "        \n",
    "        for epoch in range(self.nepochs):\n",
    "            print (epoch)\n",
    "            # Sample the hidden variables\n",
    "            print (self.compute_hidden(visible,labels))\n",
    "    \n",
    "            # Compute <vh>_0\n",
    "            positive = np.dot(inputs.T,self.hiddenact)\n",
    "            #positive = np.dot(inputs.T,self.hiddenprob)\n",
    "            positive_vb = inputs.sum(axis=0)\n",
    "            positivehb = self.hiddenprob.sum(axis=0)\n",
    "\n",
    "            print (positive, positive_vb, positivehb)\n",
    "\n",
    "            # Do limited Gibbs sampling to sample from the hidden distribution\n",
    "            for j in range(self.nCDsteps):    \n",
    "                print (self.compute_visible(self.hiddenact))\n",
    "                print (self.compute_hidden(self.visibleact,self.labelact))\n",
    "\n",
    "            # Compute <vh>_n\n",
    "            negative = np.dot(self.visibleact.T,self.hiddenact)\n",
    "            #negative = np.dot(self.visibleact.T,self.hiddenprob)\n",
    "            negativevb = self.visibleact.sum(axis=0)\n",
    "            negativehb = self.hiddenprob.sum(axis=0)\n",
    "            print (negative, negativevb, negativehb)\n",
    "\n",
    "            # Learning rule (with momentum)\n",
    "            dw = self.eta * ((positive - negative) / np.shape(inputs)[0] - self.decay*self.weights) + self.momentum*dw\n",
    "            self.weights += dw\n",
    "\n",
    "            dwvb = self.eta * (positive_vb - negativevb) / np.shape(inputs)[0] + self.momentum*dwvb\n",
    "            self.visiblebias += dwvb\n",
    "            dwhb = self.eta * (positivehb - negativehb) / np.shape(inputs)[0] + self.momentum*dwhb\n",
    "            self.hiddenbias += dwhb\n",
    "\n",
    "            error = np.sum((inputs - self.visibleact)**2)\n",
    "            if (epoch%50==0) and not silent: \n",
    "                print (epoch, error/np.shape(inputs)[0], self.energy(visible,self.hiddenprob,labels).sum())\n",
    "\n",
    "            visible = inputs\n",
    "            self.labelact = labels\n",
    "\n",
    "        if not silent:\n",
    "            self.compute_hidden(inputs,labels)\n",
    "            print (self.energy(visible,self.hiddenprob,labels).sum())\n",
    "        return error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvisible,nhidden,nlabels=None,inputs=[],eta=0.1,momentum=0.9,decay=0.,nCDsteps=5,nepochs=1000):\n",
    "\n",
    "def test_rbm3():\n",
    "    \n",
    "    #import rbm\n",
    "    # visible = 5\n",
    "    # hidden = 2\n",
    "    r = ds_rbm(5, 2,nCDsteps=1,momentum=0.9,nepochs = 100)\n",
    "    inputs = np.array(\n",
    "        [[0,1,0,1,1],[1,1,0,1,0],[1,1,0,0,1],[1,1,0,0,1], [1,0,1,0,1],[1,1,1,0,0]]\n",
    "    )\n",
    "    r.contrastive_divergence(inputs)\n",
    "    \n",
    "    #r.cddemo(inputs)\n",
    "    print (\"weights\")\n",
    "    print (r.weights)\n",
    "    print (\"visiblebias\")\n",
    "    print (r.visiblebias)\n",
    "    print (\"hiddenbias\")\n",
    "    print (r.hiddenbias)\n",
    "    print (\"hiddenact :shape[\"+str(r.hiddenact.shape)+\"]\")\n",
    "    print (r.hiddenact)\n",
    "    print (\"---\")\n",
    "    print ()\n",
    "    \n",
    "    # recall\n",
    "    test = np.array([[1,1,0,0,1]])\n",
    "    r.compute_hidden(test)\n",
    "    print (r.hiddenact)\n",
    "    print (r.hiddenact.shape)\n",
    "\n",
    "    test = np.array([[1,0]])\n",
    "    r.compute_visible(test)\n",
    "    print (r.visibleact)\n",
    "\n",
    "    test = np.array([[1,0]])\n",
    "    r.compute_visible(test)\n",
    "    print (r.visibleact)\n",
    "\n",
    "    test = np.array([[1,0]])\n",
    "    r.compute_visible(test)\n",
    "    print (r.visibleact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.0 -1.64324107641\n",
      "50 1.33333333333 -22.3809132335\n",
      "energy: [-6.24388596 -5.60775218 -8.28837841 -8.28837841 -6.31150147 -6.91637083]\n",
      "weights\n",
      "[[-1.44476295  3.71527859]\n",
      " [ 3.42942419  0.53455128]\n",
      " [-4.82306848  1.9164637 ]\n",
      " [ 1.78137862 -1.80077105]\n",
      " [ 1.09949549  0.39772464]]\n",
      "visiblebias\n",
      "[ 0.96602661  1.08846625 -0.37432778 -0.49342551  0.55176798]\n",
      "hiddenbias\n",
      "[-0.92524516 -0.8194087 ]\n",
      "hiddenact :shape[(6, 2)]\n",
      "[[ 1.  0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n",
      "---\n",
      "\n",
      "[[ 0.  1.]]\n",
      "(1, 2)\n",
      "[[ 0.  1.  0.  0.  1.]]\n",
      "[[ 1.  1.  0.  1.  1.]]\n",
      "[[ 0.  1.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "test_rbm3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = [1,0,0]\n",
    "W= np.eye(3)\n",
    "np.dot(v,W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 2, 2, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array(\n",
    "        [[0,1,0,1,1],[1,1,0,1,0],[1,1,0,0,1],[1,1,0,0,1], [1,0,1,0,1],[1,1,1,0,0]]\n",
    "    )\n",
    "inputs.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "t = \n",
    "\n",
    "k = 0 if t == 3 else 5\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
